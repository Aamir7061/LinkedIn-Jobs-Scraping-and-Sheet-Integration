{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106048ad",
   "metadata": {},
   "source": [
    "Packages that we have to install\n",
    "scrapy,scrapy-crawler,and gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dec4d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import scrapy\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf9a24",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Creating a class for scraping\n",
    "class LinkedJobsSpider(scrapy.Spider):\n",
    "    # Name of the Spider\n",
    "    name = \"linkedin_jobs\"\n",
    "\n",
    "    #LinkedIn Api url for worldwide job fetching except(USA and Canada)\n",
    "    api_url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?currentJobId=3822072449&f_CR=102890883%2C105015875%2C101282230%2C106057199%2C101165590%2C102713980%2C102890719%2C103350119%2C101620260%2C102454443%2C103291313%2C101452733%2C105646813&geoId=92000000&location=Worldwide&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R&start='\n",
    "    \n",
    "    # Starting point for the spider for crawl\n",
    "    def start_requests(self):\n",
    "        # Initial page number for job listing\n",
    "        first_job_on_page = 0\n",
    "        #Making the url for first page\n",
    "        first_url = self.api_url + str(first_job_on_page)\n",
    "        #Start the spider with a request to the initial url and specift the callback function\n",
    "        yield scrapy.Request(url=first_url, callback=self.parse_job, meta={'first_job_on_page': first_job_on_page})\n",
    "\n",
    "    # Parse job information from a specific job page\n",
    "    def parse_job(self, response):\n",
    "        first_job_on_page = response.meta['first_job_on_page']\n",
    "        #creating the empty dictionary\n",
    "        job_item = {}\n",
    "        # Extract job information using CSS selectors\n",
    "        jobs = response.css(\"li\")\n",
    "        #storing the number of job post in the page \n",
    "        num_jobs_returned = len(jobs)\n",
    "        # Extract job information using CSS selectors\n",
    "        for job in jobs:\n",
    "            #fetching the data for each job post\n",
    "            job_item['job_title'] = job.css(\"h3::text\").get(default='not-found').strip()\n",
    "            job_item['company_name'] = job.css('h4 a::text').get(default='not-found').strip()\n",
    "            job_item['company_location'] = job.css('.job-search-card__location::text').get(default='not-found').strip()\n",
    "            job_item['job_listed'] = job.css('time::text').get(default='not-found').strip()\n",
    "            job_item['job_detail_url'] = job.css(\".base-card__full-link::attr(href)\").get(default='not-found').strip()\n",
    "            job_item['company_link'] = job.css('h4 a::attr(href)').get(default='not-found')\n",
    "            # Yield the job information for further processing\n",
    "            yield job_item\n",
    "            #Function calling for storing the extracted data into the sheet\n",
    "            self.write_to_sheet(job_item)\n",
    "\n",
    "        # Check if there are more jobs on the next page\n",
    "        if num_jobs_returned > 0:\n",
    "            # Increment the page number by 25 for the next page \n",
    "            first_job_on_page = int(first_job_on_page) + 25\n",
    "            # Construct the URL for the next page\n",
    "            next_url = self.api_url + str(first_job_on_page)\n",
    "            # Make a request to the next page and specify the callback function\n",
    "            yield scrapy.Request(url=next_url, callback=self.parse_job, meta={'first_job_on_page': first_job_on_page})\n",
    "    \n",
    "    #Storing the extracted information into the google sheet\n",
    "    def write_to_sheet(self, job_item):\n",
    "        # Authenticate with Google Sheets using credentials\n",
    "        gc = gspread.service_account(filename='mycred.json')\n",
    "        # Open the specified Google Sheet by name\n",
    "        wks = gc.open('linkfind').sheet1\n",
    "        # Extract job information fields and append a new row to the Google Sheet\n",
    "        row = [\n",
    "            job_item.get('job_title', ''),\n",
    "            job_item.get('company_name', ''),\n",
    "            job_item.get('company_location', ''),\n",
    "            job_item.get('job_listed', ''),\n",
    "            job_item.get('job_detail_url', ''),\n",
    "            job_item.get('company_link', ''),\n",
    "        ]\n",
    "        # Append the row to the Google Sheet\n",
    "        wks.append_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36988cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from Scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.settings import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CrawlerProcess instance with default settings\n",
    "process = CrawlerProcess(settings=Settings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the spider named 'LinkedJobsSpider'\n",
    "process.crawl(LinkedJobsSpider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b89bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the CrawlerProcess to run the spider\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
